---
title: "NYC Crime and Real Estate Data Project Tutorial"
subtitle: "ENGM 182 - Data Analytics"
author: Vikhyat Khare, Omkar Kshirsagar, Carter Noordsij, John Sullivan
date: June 9, 2020
output:
  html_document:
    theme: paper
    number_sections: true
    highlight: kate
    toc: true
    toc_float: true
---

```{r include = FALSE}
library(tidyverse)
knitr::opts_knit$set(root.dir = "..")
```

# Background

This tutorial page describes the final project we conducted for Professor Geoffrey Parker's Spring 2020 Data Analytics course (ENGM 182) at Dartmouth College's Thayer School of Engineering. The goal of the project was to create a compelling and approachable a visualization of historical NYC real estate sales and crime data, and an accompanying model for predicting the price of a hypothetical real estate sale.

We chose to use the R programming language to complete this task. The secions below describe the code and methodologies for acquiring, cleaning, and processing the data, as well as the code that drives the Shiny app visualization.

The project's source code is available on [Github](https://github.com/sulljohn/Engm182_Project), and the Shiny app is hosted for viewing on [shinyapps.io](https://jcnoordsij.shinyapps.io/nyc_vis/). Our data sources can be downloaded at the links below:

* NYC Crimes 2006-2017: https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i
* NYC Real Estate Data 2003-2015: https://data.cityofnewyork.us/Housing-Development/NYC-Calendar-Sales-Archive-/uzf5-f8n2
* NYC Real Estate Data 2016-2017: https://www.kaggle.com/new-york-city/nyc-property-sales
* NYC Real Estate Past Year:  https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page
* NYC Census and Economic Data 2015: https://www.kaggle.com/muonneutrino/new-york-city-census-data

# Importing Data

This section describes how we imported the data from each of our data sources into an R environment, where it can be then be easily cleaned and processed. Expand the tab below to see the directory structure we used for our downloaded data files. <details><summary>Data directory structure</summary>
```{r echo = FALSE, comment=NA}
setwd("..")
fs::dir_tree(path="Data")
```
</details>

## Importing the Real Estate Data
The yearly real estate data we downloaded from the [city of New York's OpenData portal](https://opendata.cityofnewyork.us) were formatted in Excel files (with the exception of that from 2016-2017, which were contained in a single CSV file, and 2018, which were missing entirely). Each Excel file contained all the real estate sales in a particular borough during a particular calendar year from 2003 to 2020.

To import the Excel file data into R, we start by creating a list of the all the filenames.
```{r eval = FALSE}
# Get list of all excel file names for 2003-2015 and 2019-2020 data
all_files = c(
    list.files(path="../Data/RE_2003-2015", pattern="*.xls", full.names=TRUE, recursive=FALSE),
    list.files(path="../Data/RE_Apr2019-Mar2020", pattern="*.xls", full.names=TRUE, recursive=FALSE)
)
```

Next, we create a custom function to pass to `lapply()` that will import the data from each file into a dataframe, and store all those dataframes into a corresponding list called `dfs_list`. Although all the data across every file were consistently formatted with the same column headers, frustratingly, they did not always start on the same row (but always started on one of the first 10 rows). Because of this, we had to do two reads of each fileâ€”an initial read of the first 10 rows and columns just to find the index of the first column header (titled "BOROUGH" in our case), and a second read to actually import the data starting from that index. [This stack overflow post](https://stackoverflow.com/questions/43242467/reading-excel-in-r-how-to-find-the-start-cell-in-messy-spreadsheets?rq=1) was very helpful in setting up this 'double-read' functionality. We also use `clean_names()` from the `janitor` package to make the column names easier to work with later on.
```{r eval = FALSE}
library(tidyverse)
library(readxl)
library(data.table)
library(janitor)

# Import data from all files into a list of datarames
dfs_list = lapply(all_files, function(filename) {
  
    # Setting up the first read
    temp_read = suppressMessages(read_excel(filename))
    desired_sheet = 1
    skip_rows = NULL
    col_skip = 0
    search_string = "BOROUGH"
    max_cols_to_search = 10
    max_rows_to_search = 10
    
    # First read to get the cell index of the first column header
    while (length(skip_rows) == 0) {
        col_skip = col_skip + 1
        if (col_skip == max_cols_to_search) break
        skip_rows = which(stringr::str_detect(temp_read[1:max_rows_to_search,col_skip][[1]],search_string)) - 0
    }
  
    # Second read to actually import the data
    df = suppressMessages(read_excel(
        filename,
        sheet = desired_sheet,
        skip = skip_rows,
        .name_repair="universal"
    ) ) %>% clean_names()
  
    return(df)
})
```

The only data we still need to import are the 2016-2017 data from the CSV file. We append it to our list of dataframes before combining that list into a single dataframe with `rbindlist()` and saving it as in an RDA file for easy access later on.
```{r eval = FALSE}
# Append 2016-2017 data from csv file to list of data frames
dfs_list = append(dfs_list, list(read_csv("../Data/RE_2016-2017/nyc-rolling-sales.csv")[,-(1)] %>% clean_names()))

# Combine list of data frames into a single data frame and save as RDA file
df_housing = rbindlist(dfs_list)
save(df_housing, file="Data_Housing.rda")
```

Now that we have our housing data, we can move on to importing the crime data!
```{r echo = FALSE, message = FALSE, warning = FALSE}
load("Data_Housing.rda")
library(knitr)
library(kableExtra)

kable(head(df_housing, 50), caption = "<p style=\"padding-left:10px\"> **df_housing**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "300px")
```

# Data Cleaning and Processing

After importing the data, we moved on to cleaning and processing. Cleaning the data involved removing the entries that were either irrelevant or incomplete for the purposes of our project. Processing involved geocoding the data, grouping it spatiotemporally, and then computing new metrics to help describe each group.

Grouping the data would allow us to more easily draw observe trends and draw meaningful conclusions, and we decided to group the real estate and crime data entries by zip code and month. The ~250 zip code areas in NYC would give us enough granularity to see the how the myriad of different regions in New York City differ from one another over time, but also enough courseness to guarantee acceptable response times of our visualization app, and ensure that there were usually multiple entries in each group to average across.

## Group crime information by month, year, and zipcode

First, the relevant datafiles are imported into the environment and the packages are loaded:

```{r eval=FALSE}
library(lubridate)
library(dplyr)
load(file = "Data_Crime_w_Zipcodes.rda")
load(file = "Data_Zip_Population.rda")
```

For analysis, it is helpful to have the events grouped by month, year, and zipcode. Then, a field like the crime score can be aggregated and normalized for this criterion. In this case, the crime data is formatted in the desired way with the following command:

```{r eval=FALSE}
score_by_time_and_rating <- df_crime %>% group_by(month=floor_date(CMPLNT_FR_DT, "month"), zip_code)  %>% summarize(summary_variable=sum(weight))
```

Source: https://stackoverflow.com/questions/33221425/how-do-i-group-my-date-variable-into-month-year-in-r

### Group other fields by time and zipcode

In addition, commands are run to normalize the crime score by the total population and only take data greater than the year 2000. There are extraneous entries in the data for crimes hundreds of years ago and 2000 is a good baseline. It also appears that the reporting method for these crimes changes in 2006, as evidenced by a dramatic change in crime scores at this time.

```{r class.source = NULL, eval = FALSE}
# Summing the scores
names(score_by_time_and_rating)[names(score_by_time_and_rating) == 'summary_variable'] <- 'sum_weight'

# Joining the zipcodes
score_by_time_and_rating <- left_join(score_by_time_and_rating, zip_population, by="zip_code")

# Dividing sum by the total population (at 2015 census levels)
score_by_time_and_rating$weight <- score_by_time_and_rating$sum_weight / score_by_time_and_rating$TotalPop

# Dipslaying the head
head(score_by_time_and_rating)

save(score_by_time_and_rating, file = "Data_Score_by_Time_and_Rating.rda")

#Group by year and rating (This is for the regression model for house sales)
score_by_zip_and_year <- df_crime %>% group_by(year=floor_date(CMPLNT_FR_DT, "year"), zip_code)  %>% summarize(summary_variable=sum(weight))

#Summing the scores
names(score_by_zip_and_year)[names(score_by_zip_and_year) == 'summary_variable'] <- 'sum_weight'

#Extracting the year
score_by_zip_and_year$year <- year(score_by_zip_and_year$year)
score_by_zip_and_year <- subset(score_by_zip_and_year, select = c(year,zip_code,sum_weight))

# Joining the zipcodes with populations
score_by_zip_and_year <- left_join(score_by_zip_and_year, zip_population, by="zip_code")
score_by_zip_and_year <- subset(score_by_zip_and_year, select = c(year,zip_code,sum_weight, TotalPop))

# Dividing sum by the total population (at 2015 census levels)
score_by_zip_and_year$weight <- score_by_zip_and_year$sum_weight / score_by_zip_and_year$TotalPop

#Taking data only for years 2001 and above 
score_by_zip_and_year <- subset(score_by_zip_and_year, year > 2000)
score_by_zip_and_year <- subset(score_by_zip_and_year, select = c(zip_code, year, sum_weight, weight, TotalPop))

save(score_by_zip_and_year, file = "Data_Score_by_year_and_zipcode.rda")
```

## Converting coordinates to zipcodes

There was difficulty with initially processing the location data into a format for analysis. For the crime and census data:

* Crime data - complaints are recorded with only a latitude and longitude
* Census data - the census data uses census tracts for identifying locations

For the regressions and displaying the data visually, the data needs to be grouped by similar location. In this case, zipcodes were chosen to be the level to group by. By classifying the data entries in a zipcode, the effect of the zipcode can be studied.

At first, an API was used to convert the coordinates from the crime data to zip codes. The revgeocode function from the ggmap package was used and applied to each crime complaint. However, it was taking too long to make the requests to retrieve each of the zipcodes. Although this route was not pursued, it may be possible to perform the API requests in parallel and improve performance.

Instead, it was chosen to perform the reverse geocoding locally and avoid having to make requests to an API service for each zip code. Further, since the zip codes are limited to only New York, it was possible to only download the location data for this area. In this section, the same method will be applied to extract the zipcodes.

### Sources for latitude and longitude to zipcode code and data

The source for the code to retrieve the zipcodes from latitude and longitude coordinates locally is: https://stackoverflow.com/questions/46267287/reverse-geocoding-speed

The source for the geospatial polygon data to process the coordinates is: https://jsspina.carto.com/tables/nyc_zip_code_tabulation_areas_polygons/public/map

### Processing the crime data

First, the packages are loaded:

```{r eval=FALSE}
library(sf)
library(purrr)
library(lubridate)
library(dplyr)
```

The crime data is read in with the following command:

```{r eval=FALSE}
sf <- sf::st_read("nyc_zip_code_tabulation_areas_polygons.geojson")
```

Then the desired latitude and longitude fields are taken. the complaint number is also taken so that the crime-zipcode data can be joined again later. The NA values are omitted.

```{r eval=FALSE}
CMPLNT_NUM <- c(df_crime$CMPLNT_NUM)
latitude <- c(df_crime$Latitude)
longitude <- c(df_crime$Longitude)

x = data.frame(CMPLNT_NUM, longitude, latitude)
x <- na.omit(x)

sf_x <- sf::st_as_sf(x, coords = c("longitude", "latitude"))
```

As stated in the sourced guide above for reverse geocoding this data, "To perform spatial operations, the coordinate reference system needs to match between the two geometries." 

```{r eval=FALSE}
st_crs(sf_x) <- st_crs(sf)
```

The coordinates are processed with the st_within function and put into an output using sapply:

```{r eval=FALSE}
res <- st_within(sf_x, sf)

out <- sapply(res, function(x) as.character(sf$postalcode[x]))
```

The output for each zipcode was a list of zipcodes even though there was just one. There were multiple outputs in the case that one was trying to link overlapping geometries with the data. The zipcodes are converted to the proper format:

```{r eval=FALSE}
out2 <- map(out, 1) # Take first element from each list
out3 <- lapply(out, function(x) ifelse(length(x) == 1, x[1], NA)) # Handle NAs
out4 <- vapply(out3, paste, collapse = ", ", character(1L)) # Flattten listst; source: https://stackoverflow.com/questions/24829027/unimplemented-type-list-when-trying-to-write-table

sf_x$zip_code <- out4
```

Finally, the zipcodes are linked back with the crime data and saved:

```{r eval=FALSE}
# Putting it back with the sf_data
df_crime <- left_join(df_crime, sf_x, by = "CMPLNT_NUM")

# Format column as dates
df_crime$CMPLNT_FR_DT <- as.Date(df_crime$CMPLNT_FR_DT, format = "%m/%d/%Y")

# This column NEEDS to be removed because it causes errors
df_crime$geometry<-NULL

# Saving the data with thet important information
save(df_crime, file='Data_Crime_w_Zipcodes.rda')
```

### Processing the census data

First, the packages are loaded:

```{r eval=FALSE}
library(readxl)
library(dplyr)
library(plyr)
```

The process for finding the zipcodes that corresponded to the census data is different. The census tract information is more granular than at the zipcode level. The mapping between tracts and zipcodes is imported and joined to the census data, so that each element has a corresponding zipcode. The Excel file can be downloaded from the source at end of this section.

```{r eval=FALSE}
tract_zips <- my_data <- read_excel("TRACT_ZIP_032020.xlsx")

names(tract_zips)[names(tract_zips) == 'ZIP'] <- 'zip_code'
names(tract_zips)[names(tract_zips) == 'TRACT'] <- 'CensusTract'
tract_zips$CensusTract <- as.numeric(tract_zips$CensusTract)

nyc_tracts <- left_join(nyc_tracts, tract_zips, by="CensusTract")

# Save updated nyc_tracs
save(nyc_tracts, file='Data_NYC_Tracts.rda')

#Adding all the parameters required for regression in zip_population file
zip_population <- subset(nyc_tracts, select = c(CensusTract,zip_code,TotalPop, Men, Women, Hispanic, White, Black, Native, Asian, IncomePerCap, Unemployment))
```

The most important element was the corresponding populations for each zipcode. The population data for each demographic was summed across each zipcode. The demographic information was initially stored as percentages, so it was necessary to multiply those percentages by the total population for each census tract.

```{r eval=FALSE}
#Converting the demographic and unemployment percentages to numbers and income per capita to income
zip_population$Hispanic.number <- zip_population$Hispanic*zip_population$TotalPop/100
zip_population$White.number <- zip_population$White*zip_population$TotalPop/100
zip_population$Black.number <- zip_population$Black*zip_population$TotalPop/100
zip_population$Native.number <- zip_population$Native*zip_population$TotalPop/100
zip_population$Asian.number <- zip_population$Asian*zip_population$TotalPop/100
zip_population$IncomeTot <- zip_population$IncomePerCap*zip_population$TotalPop
#Percentage of people in the age group of 18-65 in NY is 65% source: https://www.census.gov/quickfacts/newyorkcitynewyork
zip_population$Unemployment.number <- zip_population$Unemployment*0.65*zip_population$TotalPop/100

#Aggregating the various census tract data into 1 zip code
zip_population <- subset(zip_population, select = c(zip_code,TotalPop, Men, Women, Hispanic.number, White.number, Black.number, Native.number, Asian.number, IncomeTot, Unemployment.number))
zip_population <- aggregate(.~zip_code, data = zip_population, FUN = sum)

#Convert back the demographics and unemployment to % and IncomeTot to per capita income
zip_population$Hispanic.number <- zip_population$Hispanic.number*100/zip_population$TotalPop
zip_population$White.number <- zip_population$White.number*100/zip_population$TotalPop
zip_population$Black.number <- zip_population$Black.number*100/zip_population$TotalPop
zip_population$Native.number <- zip_population$Native.number*100/zip_population$TotalPop
zip_population$Asian.number <- zip_population$Asian.number*100/zip_population$TotalPop
zip_population$IncomeTot <- zip_population$IncomeTot/zip_population$TotalPop
zip_population$Unemployment.number <- zip_population$Unemployment.number*100/(0.65*zip_population$TotalPop)
```

Finally, the columns are renamed as needed. The data for zipcode to population demographic was saved for analysis later:

```{r eval=FALSE}
#Renaming the column headers
colnames(zip_population) <- c("zip_code", "TotalPop", "Men", "Women", "Hispanic", "White", "Black", "Native", "Asian", "PerCapitaIncome", "Unemployed")

save(zip_population, file = "Data_Zip_Population.rda")
```

### Source for census tract to zipccode mapping data

The following site includes the most up to date census tract to zipcode information: https://www.huduser.gov/portal/datasets/usps_crosswalk.html


## Cleaning the Crime Data

Generally, the crime data is cleaned as part of the other sections. When the crime scores are taken to classify the crimes and these scores are summed across zipcodes, crimes that could not be classified into defined categories are excluded. While excluding NAs could introduce bias, it is believed that the effect would be consistent across time and population for the analysis. However, it did appear that there was a difference in the reporting for crime scores after 2006, which was accounted for when representing the data visually.

## Geocoding the Real Estate Data

In order to group the real estate data by zip code, we first need to attach a zip code value to each entry in our data (a process known as "geocoding", or "reverse-geocoding" to be more precise). To do this, we use the [Geoclient API](https://developer.cityofnewyork.us/api/geoclient-api) available on the city's [Developer Portal](https://developer.cityofnewyork.us). Before pulling from the API, we do some quick cleaning by converting the `borough`, `sale_price`, and `gross_square_feet` columns to the `numeric` data type (to make sure they're not incorrectly stored as `character` or `factor` types). Then we filter out some of the data with bad `sale_price` and `gross_square_feet` values, as this makes for fewer API queries and we know we won't want use those entries in our analysis anyway. The last step before pulling from the API is creating the `bbl` column, which is a 10 digit combination of the `borough`, `block`, and `lot` columns. We will pass each entry's `bbl` number to the API, which will then return the zip code (among other data) for that specific plot of land.

```{r eval = FALSE}
library(tidyverse)

load("Data_Housing.rda")

# Convert chr variables to numerics
df_housing$borough = as.numeric(df_housing$borough)
df_housing$sale_price = as.numeric(df_housing$sale_price)
df_housing$gross_square_feet = as.numeric(df_housing$gross_square_feet)

# Get only data with valid sale prices and square footages
# Build borough-block-lot (BBL) ID for reverse geocoding
df_housing = df_housing %>%
  filter(sale_price > 0) %>%
  filter(gross_square_feet > 1) %>%
  mutate(bbl = borough * 1e9 + block * 1e4 + lot)
```

A nifty CRAN package appropriately titled [`geoclient`](https://rdrr.io/github/austensen/geoclient/man/geoclient-package.html) allows us to pull data from the Geoclient API in just a single line of code. However, because the API is public and free, the rate of requests from a single account is limited and it took ~7 hours to query all 1.4 mllion properties. After the queries are complete, we bind the returned API data to the existing `df_housing` data frame in a new data frame called `geocoded_df_housing`.

```{r eval = FALSE}
library(geoclient)

# Query the API
bbl_df = geo_bbl(df_housing$bbl, id = APPLICATION_ID, key = API_KEY)

# Join the existing data with the API data
geocoded_df_housing = bind_cols(df_housing, bbl_df)
```

## Cleaning the Real Estate Data

In the next step, we modify data from a CSV file with all NYC building class codes to include a column labeling the building category of each class code (e.g., Residential, Commercial, Industrial, etc.).

```{r, message=FALSE, warning=FALSE}
building_class_key = read_csv("BUILDING_CLASS.csv") %>%
  group_by(BUILDING_CODE_ID) %>%
  summarize(category = str_to_title(TYPE[1])) %>%
  rename(class_code = BUILDING_CODE_ID) %>%
  mutate(category = case_when(
    substring(class_code, 1,1) %in% c("A","B","C","D","R") ~ "Residential",
    substring(class_code, 1,1) %in% c("S") ~ "Mixed",
    substring(class_code, 1,1) %in% c("H","K","L","O") ~ "Commercial",
    substring(class_code, 1,1) %in% c("E","F","G","T","U") ~ "Industrial",
    substring(class_code, 1,1) %in% c("V") ~ "Vacant",
    substring(class_code, 1,1) %in% c("I","J","M","N","P","Q","W") ~ "Civic",
    substring(class_code, 1,1) %in% c("Y") ~ "Government",
    substring(class_code, 1,1) %in% c("Z") ~ "Other"
  ))
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(kableExtra)

kable(building_class_key, caption = "<p style=\"padding-left:10px\"> **building_class_key**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "30%", height = "300px")
```
<br/>
Now we select the columns from `geocoded_df_housing` that we want to work with moving forward, filter out missing values and high/low-priced outliers, and use the building class codes to assign a category to each building sale. This is all stored and saved new data frame called `cleaned_df_housing`, which we will use for grouping in the next sub-section.

```{r, eval = FALSE}
cleaned_df_housing = geocoded_df_housing %>%
  select(
    building_class_at_present,
    zip_code,
    gross_square_feet,
    sale_price,
    sale_date,
  ) %>%
  rename(lat = latitudeInternalLabel) %>%
  rename(lng = longitudeInternalLabel) %>%
  filter(!(is.na(lat) | is.na(lng))) %>%
  filter(!(is.na(zip_code) | zip_code == 0)) %>%
  filter(!(is.na(sale_price))) %>%
  filter(!(is.na(gross_square_feet) | gross_square_feet <= 0)) %>%
  mutate(price_per_sqft = sale_price/gross_square_feet) %>%
  filter(price_per_sqft <= 2500) %>%
  filter(price_per_sqft > 20) %>%
  mutate(zip_code = sapply(zip_code, as.character)) %>%
  left_join(building_class_key,  by = c("building_class_at_present" = "class_code"))

save(cleaned_df_housing, file="cleaned_housing.rda")
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(kableExtra)
load("cleaned_housing.rda")
kable(head(cleaned_df_housing, 50), caption = "<p style=\"padding-left:10px\"> **cleaned_df_housing**</p>") %>%
    kable_styling() %>%
    column_spec(5, width_min = "90px") %>%
    scroll_box(width = "100%", height = "300px")
```

<br/>

## Grouping the Real Estate Data

Now that we have the real estate geocoded and cleaned, we can group it by aggregating all sales that took place in the same zip code and during the same month. We will create 3 new variables to describe each group: average price per square foot (`avg_price_per_sqft`), total number of sales (`num_sales`), and total proceeds from all sales (`total_proceeds`).
```{r}
load("cleaned_housing.rda")

grouped_housing_all = cleaned_df_housing %>%
    mutate(month_char = format(as.Date(sale_date), "%Y-%m")) %>%
    group_by(zip_code, month_char) %>%
    summarize(
        category = "All",
        avg_price_per_sqft = mean(price_per_sqft),
        num_sales = n(),
        total_proceeds = sum(sale_price)
    )
```

Next, we peform the same grouping function again, but with building category as an extra grouping variable. This will allow us to filter the displayed data by building category in the UI for our visualization. We combine the groups of individual categories with the groups of all categories to create a single data frame. Then we filter high-sales outliers and save this data frame for visualization in our Shiny app.

```{r}
grouped_housing = cleaned_df_housing %>%
    mutate(month_char = format(as.Date(sale_date), "%Y-%m")) %>%
    group_by(zip_code, month_char, category) %>%
    summarize(
        avg_price_per_sqft = mean(price_per_sqft),
        num_sales = n(),
        total_proceeds = sum(sale_price)
    ) %>%
    bind_rows(grouped_housing_all) %>%
    filter(total_proceeds < 200000000) %>%
    data.frame()

save(grouped_housing, file="grouped_housing.rda")
```

```{r echo = FALSE}
library(knitr)
library(kableExtra)
kable(head(grouped_housing, 50), caption = "<p style=\"padding-left:10px\"> **grouped_housing**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "300px")
```
<br/>

# Regression Modeling

The regression model helps to establish a baseline. Unfortunately, when working with a large set of data, there are issues faced when running regressions:

1. Large dataset - it takes a long time to run the regression
2. P-values - they go to zero quickly (they all became significant)
3. Mean-squared error (MSE) - the mean square errors of the predicted values are high, despite the amount of data

Each of the issues can be respectively addressed:

1. A function called fastLm removes overhead from the regression
2. The data can be partitioned, and the accuracy prediction can be analyzed (MSE)
3. Machine learning models can be used to capture data granularity (see machine learning code section)

## Code to setup and run regressions on combined crime data

The steps for setting up the regression, running the model, and making predictions are in the following sections.

### Load packages and import data

First, the packages are loaded:

```{r eval=FALSE}
library(dplyr)
library(RcppArmadillo)
library(lubridate)
```

The desired data is imported into R for the regression.

```{r eval=FALSE}
#Regress sale price with sale year, land area, gross area, tax class, building class, year built
load(file = "Data_sale_census_crime.rda")
```

### Variable selection

The desired independent and dependent (sale_price) variables are selected with the code below and certain parameters are set to limit the amount of data.

```{r eval=FALSE}
# Make sure sale prices are numeric
df_sale_census_crime$sale_price <- as.numeric(df_sale_census_crime$sale_price)

# Remove large house sale prices; source: https://stackoverflow.com/questions/25764810/delete-rows-based-on-range-of-values-in-column
df_sale_census_crime <- df_sale_census_crime[with(df_sale_census_crime, sale_price <= 1000000), ]

#Regression 1
x <- df_sale_census_crime %>% select("land_square_feet", "PerCapitaIncome", "Unemployed", "TotalPop.x", "Men", "Women")

y <- df_sale_census_crime %>% select("sale_price")
```

### Variable coercion

The variables are coerced into the proper types for the regression (e.g. factor variable).

```{r eval=FALSE}
x$land_square_feet <- as.numeric(x[[1]])
x$PerCapitaIncome <- as.numeric(x[[2]])
x$Unemployed <- as.numeric(x[[3]])
x$TotalPop.x <- as.numeric(x[[4]])
x$Men <- as.numeric(x[[5]])
x$Women <- as.numeric(x[[6]])

# Setting up factor variable
df_sale_census_crime$sale_year <- as.character(df_sale_census_crime$sale_year)
df_sale_census_crime$year_built <- as.character(df_sale_census_crime$year_built)

building_class <- model.matrix( ~ building_class_at_time_of_sale - 1, data=df_sale_census_crime )
sale_year <- model.matrix( ~ sale_year - 1, data=df_sale_census_crime )
year_built <- model.matrix( ~ year_built - 1, data=df_sale_census_crime )

# Binding factor variables
x<-cbind(x, building_class)
x<-cbind(x, sale_year)
x<-cbind(x, year_built)

# Add intercept column and renaming
x<-cbind(x, 1)
colnames(x)[dim(x)[2]] <- "Intercept" # Source: https://www.dummies.com/programming/r/how-to-name-matrix-rows-and-columns-in-r/

# Setup the y output variable
y <- as.numeric(y[[1]])

# Display the classes
sapply(x, class)
sapply(y, class)
```

### Partition the data

The data is partitioned into training and test sets for running regressions and determining the accuracy of a model on the test set respectively. For this initial regression analysis, a subset of 80% of the data is used for training and the rest is used for testing the accuracy of the predictions.

```{r eval=FALSE}
# ----- Split data into training and validation sets ----- 
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- sort(sample(nrow(x), nrow(x)*.8))
# select 20% of the data for validation
x_test <- x[-validation_index,]
y_test <- y[-validation_index]
# use the remaining 80% of data to training and testing the models
x_train <- x[validation_index,]
y_train <- y[validation_index]
```

### Run the regression using fastLm

The regression is run using fastLm to expedite the process and remove the overhead from the normal regression functions.

```{r eval=FALSE}
# Run regression
fit1 <- fastLm(x_train, y_train)
```

### Summarize results

The results are summarized for analysis.

```{r eval=FALSE}
summary(fit1)
#R-squared value is 25.63%

# Perform predictions on test set
reg_pred1 <- predict(fit1, as.matrix(x_test))

# See the comparison
results <- data.frame(cbind(y_test, reg_pred1))
results$diff <- results$reg_pred1 - results$y_test
results$diff_squared <- '^' (results$diff, 2)
# RMS = 
rms = sqrt(mean(results$diff_squared))
rms

# RMS Error = 
rms_error = rms/mean(results$y_test)
rms_error
```

## Results from the regression models

The results that were found when the regression was run for this project are summarized:

| Measure                          | Value     |
|----------------------------------|-----------|
| Adjusted R-squared value ($R^2$) | 27.26%    |
| Root Mean Squared (RMS)          | 198,239.1 |
| Root Mean Squared Error (RMSE)   | 43.59%    |

The more detailed output from the regression model is summarized below. While it may be possible to improve the regression results by running further regressions, the machine learning models explored next will likely help to capture the granularity in the data.

<details>
<summary>Regression output</summary>
```
> # Run regression
> fit1 <- fastLm(x_train, y_train)
> summary(fit1)

Call:
fastLm.default(X = x_train, y = y_train)

Residuals:
     Min.   1st Qu.    Median   3rd Qu.      Max. 
-951240.0  -99101.0    2810.9  110820.0 1088100.0 

                                    Estimate      StdErr     t.value   p.value    
land_square_feet                  2.8982e-02  7.3239e-03  3.9572e+00 7.583e-05 ***
PerCapitaIncome                   1.5932e+00  5.5788e-02  2.8558e+01 < 2.2e-16 ***
Unemployed                       -1.2420e+04  1.6778e+02 -7.4027e+01 < 2.2e-16 ***
TotalPop.x                       -6.1569e+09  2.5369e-01 -2.4269e+10 < 2.2e-16 ***
Men                               6.1569e+09  2.4254e-01  2.5385e+10 < 2.2e-16 ***
Women                             6.1569e+09  2.9053e-01  2.1192e+10 < 2.2e-16 ***
building_class_at_time_of_saleA0  1.3952e+05  3.7534e+03  3.7171e+01 < 2.2e-16 ***
building_class_at_time_of_saleA1  1.0504e+05  1.4056e+03  7.4727e+01 < 2.2e-16 ***
building_class_at_time_of_saleA2  6.4862e+04  1.7563e+03  3.6931e+01 < 2.2e-16 ***
building_class_at_time_of_saleA3  3.3492e+05  5.4030e+03  6.1988e+01 < 2.2e-16 ***
building_class_at_time_of_saleA4  1.9749e+05  6.5229e+03  3.0276e+01 < 2.2e-16 ***
building_class_at_time_of_saleA5  3.9112e+04  1.4470e+03  2.7030e+01 < 2.2e-16 ***
building_class_at_time_of_saleA6 -1.5064e+05  7.0867e+03 -2.1257e+01 < 2.2e-16 ***
building_class_at_time_of_saleA7  1.8340e+05  5.3564e+01  3.4239e+03 < 2.2e-16 ***
building_class_at_time_of_saleA8 -1.1559e+05  1.4973e+01 -7.7196e+03 < 2.2e-16 ***
building_class_at_time_of_saleA9  6.3681e+04  2.0931e+03  3.0424e+01 < 2.2e-16 ***
building_class_at_time_of_saleB1  1.7355e+05  1.5154e+03  1.1453e+02 < 2.2e-16 ***
building_class_at_time_of_saleB2  1.5230e+05  1.4673e+03  1.0380e+02 < 2.2e-16 ***
building_class_at_time_of_saleB3  1.3706e+05  1.5737e+03  8.7097e+01 < 2.2e-16 ***
building_class_at_time_of_saleB9  1.2594e+05  2.0994e+03  5.9986e+01 < 2.2e-16 ***
building_class_at_time_of_saleC0  2.1164e+05  1.5671e+03  1.3505e+02 < 2.2e-16 ***
building_class_at_time_of_saleC1  2.3698e+05  4.3097e+03  5.4987e+01 < 2.2e-16 ***
building_class_at_time_of_saleC2  1.9560e+05  2.9009e+03  6.7429e+01 < 2.2e-16 ***
building_class_at_time_of_saleC3  2.1607e+05  2.6559e+03  8.1355e+01 < 2.2e-16 ***
building_class_at_time_of_saleC4  1.3695e+05  9.0729e+03  1.5095e+01 < 2.2e-16 ***
building_class_at_time_of_saleC5  1.9362e+05  6.7181e+03  2.8821e+01 < 2.2e-16 ***
building_class_at_time_of_saleC6 -1.3938e+05  3.8236e+02 -3.6451e+02 < 2.2e-16 ***
building_class_at_time_of_saleC7  1.6761e+05  7.7880e+03  2.1522e+01 < 2.2e-16 ***
building_class_at_time_of_saleC8 -3.9534e+05  3.8622e+00 -1.0236e+05 < 2.2e-16 ***
building_class_at_time_of_saleC9 -5.3525e+04  1.4023e+02 -3.8170e+02 < 2.2e-16 ***
building_class_at_time_of_saleCM -1.0373e+01          NA          NA        NA    
building_class_at_time_of_saleD0  8.2579e+04  2.5786e+01  3.2024e+03 < 2.2e-16 ***
building_class_at_time_of_saleD1 -3.0391e+04  4.1447e+02 -7.3326e+01 < 2.2e-16 ***
building_class_at_time_of_saleD2 -4.5215e+05  7.4887e+00 -6.0378e+04 < 2.2e-16 ***
building_class_at_time_of_saleD3 -1.8254e+05  9.2733e+01 -1.9684e+03 < 2.2e-16 ***
building_class_at_time_of_saleD4 -1.6205e+05  7.7115e+03 -2.1014e+01 < 2.2e-16 ***
building_class_at_time_of_saleD5  9.2455e+04  7.8430e+00  1.1788e+04 < 2.2e-16 ***
building_class_at_time_of_saleD6 -1.5591e+05  6.7311e+01 -2.3162e+03 < 2.2e-16 ***
building_class_at_time_of_saleD7 -1.0718e+05  9.0214e+01 -1.1881e+03 < 2.2e-16 ***
building_class_at_time_of_saleD8 -2.3003e+04  1.2368e+01 -1.8598e+03 < 2.2e-16 ***
building_class_at_time_of_saleD9 -1.9105e+05  7.6408e+01 -2.5004e+03 < 2.2e-16 ***
building_class_at_time_of_saleE1  1.5457e+05  4.4341e+02  3.4859e+02 < 2.2e-16 ***
building_class_at_time_of_saleE2  4.3728e+05  1.0070e+01  4.3426e+04 < 2.2e-16 ***
building_class_at_time_of_saleE3  1.4641e+05  3.0501e+02  4.8002e+02 < 2.2e-16 ***
building_class_at_time_of_saleE4  1.0907e+05  5.2813e+01  2.0653e+03 < 2.2e-16 ***
building_class_at_time_of_saleE7 -2.9949e+05  1.9951e+01 -1.5011e+04 < 2.2e-16 ***
building_class_at_time_of_saleE9  1.1786e+05  8.8237e+03  1.3357e+01 < 2.2e-16 ***
building_class_at_time_of_saleF1  1.5106e+05  1.6581e+02  9.1102e+02 < 2.2e-16 ***
building_class_at_time_of_saleF2  2.1667e+05  3.0781e+01  7.0391e+03 < 2.2e-16 ***
building_class_at_time_of_saleF4  1.6584e+05  6.3631e+02  2.6062e+02 < 2.2e-16 ***
building_class_at_time_of_saleF5  9.9344e+04  1.5017e+02  6.6155e+02 < 2.2e-16 ***
building_class_at_time_of_saleF8 -2.8220e+05  1.3758e+00 -2.0512e+05 < 2.2e-16 ***
building_class_at_time_of_saleF9  1.5692e+05  8.5776e+03  1.8295e+01 < 2.2e-16 ***
building_class_at_time_of_saleG0 -7.4920e+04  9.3074e+02 -8.0494e+01 < 2.2e-16 ***
building_class_at_time_of_saleG1  1.0360e+05  1.7836e+02  5.8084e+02 < 2.2e-16 ***
building_class_at_time_of_saleG2  1.3491e+05  7.8585e+03  1.7168e+01 < 2.2e-16 ***
building_class_at_time_of_saleG3  2.1477e+05  1.5108e+01  1.4215e+04 < 2.2e-16 ***
building_class_at_time_of_saleG4  2.3700e+05  1.2326e+02  1.9228e+03 < 2.2e-16 ***
building_class_at_time_of_saleG5  1.4077e+05  5.4218e+01  2.5963e+03 < 2.2e-16 ***
building_class_at_time_of_saleG6  1.1741e+05  4.5981e+01  2.5535e+03 < 2.2e-16 ***
building_class_at_time_of_saleG7 -1.6497e+04  4.4057e+03 -3.7444e+00 0.0001808 ***
building_class_at_time_of_saleG8  3.1372e+03  2.6921e+01  1.1653e+02 < 2.2e-16 ***
building_class_at_time_of_saleG9  1.2263e+05  7.6113e+03  1.6111e+01 < 2.2e-16 ***
building_class_at_time_of_saleGU  2.8203e+05  2.6368e+00  1.0696e+05 < 2.2e-16 ***
building_class_at_time_of_saleGW  2.0901e+05  9.7865e+00  2.1357e+04 < 2.2e-16 ***
building_class_at_time_of_saleH1 -1.7314e+00  2.4147e-12 -7.1701e+11 < 2.2e-16 ***
building_class_at_time_of_saleH2 -5.1859e+05  4.9986e+03 -1.0375e+02 < 2.2e-16 ***
building_class_at_time_of_saleH3 -5.6200e+05  8.2590e+03 -6.8046e+01 < 2.2e-16 ***
building_class_at_time_of_saleH4  1.6690e+05  1.9417e+01  8.5958e+03 < 2.2e-16 ***
building_class_at_time_of_saleH6  1.1853e+04  5.5558e+00  2.1334e+03 < 2.2e-16 ***
building_class_at_time_of_saleH7 -2.1248e+05  1.0631e+01 -1.9987e+04 < 2.2e-16 ***
building_class_at_time_of_saleH8  7.7614e-01          NA          NA        NA    
building_class_at_time_of_saleH9  2.4038e+04  2.7452e+01  8.7564e+02 < 2.2e-16 ***
building_class_at_time_of_saleHR  2.6907e+05  2.6293e+01  1.0234e+04 < 2.2e-16 ***
building_class_at_time_of_saleI1 -4.2960e+05  1.5147e+01 -2.8361e+04 < 2.2e-16 ***
building_class_at_time_of_saleI3  2.0919e+05  2.7345e+00  7.6501e+04 < 2.2e-16 ***
building_class_at_time_of_saleI4 -5.5951e+04  1.7755e+01 -3.1513e+03 < 2.2e-16 ***
building_class_at_time_of_saleI5  1.7474e+05  5.9982e+01  2.9132e+03 < 2.2e-16 ***
building_class_at_time_of_saleI6  1.1961e+05  2.4592e+01  4.8636e+03 < 2.2e-16 ***
building_class_at_time_of_saleI7  6.3836e+04  1.0070e+02  6.3393e+02 < 2.2e-16 ***
building_class_at_time_of_saleI9  8.6528e+04  4.7252e+01  1.8312e+03 < 2.2e-16 ***
building_class_at_time_of_saleJ1 -2.8717e+05  9.3349e+00 -3.0763e+04 < 2.2e-16 ***
building_class_at_time_of_saleJ2 -4.9867e+05  5.4591e+00 -9.1348e+04 < 2.2e-16 ***
building_class_at_time_of_saleJ4 -9.4846e+04  2.5902e+01 -3.6618e+03 < 2.2e-16 ***
building_class_at_time_of_saleJ6 -5.4367e+05  2.1236e+01 -2.5602e+04 < 2.2e-16 ***
building_class_at_time_of_saleJ7 -4.6257e+05  3.0157e+00 -1.5339e+05 < 2.2e-16 ***
building_class_at_time_of_saleJ9 -8.2693e-01          NA          NA        NA    
building_class_at_time_of_saleK1  1.3066e+05  5.0570e+03  2.5838e+01 < 2.2e-16 ***
building_class_at_time_of_saleK2  1.8075e+05  8.3157e+03  2.1736e+01 < 2.2e-16 ***
building_class_at_time_of_saleK3 -4.7864e+05  1.6826e+00 -2.8447e+05 < 2.2e-16 ***
building_class_at_time_of_saleK4  2.0110e+05  7.0629e+03  2.8473e+01 < 2.2e-16 ***
building_class_at_time_of_saleK5  1.5896e+05  1.5770e+02  1.0080e+03 < 2.2e-16 ***
building_class_at_time_of_saleK6  5.7383e+04  1.8493e+01  3.1029e+03 < 2.2e-16 ***
building_class_at_time_of_saleK7  1.8497e+05  4.8547e+01  3.8101e+03 < 2.2e-16 ***
building_class_at_time_of_saleK8 -3.5744e+05  2.2884e+01 -1.5620e+04 < 2.2e-16 ***
building_class_at_time_of_saleK9  1.6681e+05  8.5161e+03  1.9588e+01 < 2.2e-16 ***
building_class_at_time_of_saleL1 -3.2798e+05  8.1924e+01 -4.0035e+03 < 2.2e-16 ***
building_class_at_time_of_saleL2 -4.6219e+05  2.0444e+01 -2.2608e+04 < 2.2e-16 ***
building_class_at_time_of_saleL3 -1.0495e+00          NA          NA        NA    
building_class_at_time_of_saleL8 -7.9144e+04  1.0960e+02 -7.2209e+02 < 2.2e-16 ***
building_class_at_time_of_saleL9  4.3745e+04  3.5515e+01  1.2318e+03 < 2.2e-16 ***
building_class_at_time_of_saleM1  1.5553e+05  6.4322e+02  2.4180e+02 < 2.2e-16 ***
building_class_at_time_of_saleM2  4.7448e+05  1.3285e+00  3.5715e+05 < 2.2e-16 ***
building_class_at_time_of_saleM3  1.8350e+05  2.2997e+01  7.9795e+03 < 2.2e-16 ***
building_class_at_time_of_saleM4  3.5926e+05  1.0860e+01  3.3079e+04 < 2.2e-16 ***
building_class_at_time_of_saleM9  1.7297e+05  1.9606e+02  8.8223e+02 < 2.2e-16 ***
building_class_at_time_of_saleN1 -2.5565e+05  1.2544e+00 -2.0381e+05 < 2.2e-16 ***
building_class_at_time_of_saleN2 -1.0812e+05  3.7291e+01 -2.8993e+03 < 2.2e-16 ***
building_class_at_time_of_saleN3  3.8823e+05  1.4154e+00  2.7429e+05 < 2.2e-16 ***
building_class_at_time_of_saleN4  2.5708e+05  2.3806e+00  1.0799e+05 < 2.2e-16 ***
building_class_at_time_of_saleN9  7.9835e+04  4.0318e+01  1.9802e+03 < 2.2e-16 ***
building_class_at_time_of_saleO1  9.8493e+04  5.4205e+01  1.8170e+03 < 2.2e-16 ***
building_class_at_time_of_saleO2  5.6520e+04  8.4064e+01  6.7235e+02 < 2.2e-16 ***
building_class_at_time_of_saleO3 -2.5890e+05  7.4544e+01 -3.4732e+03 < 2.2e-16 ***
building_class_at_time_of_saleO4 -2.5858e+05  4.7554e+01 -5.4377e+03 < 2.2e-16 ***
building_class_at_time_of_saleO5  2.0678e+05  6.5123e+01  3.1753e+03 < 2.2e-16 ***
building_class_at_time_of_saleO6 -1.8394e+05  1.6759e+02 -1.0975e+03 < 2.2e-16 ***
building_class_at_time_of_saleO7  1.5411e+05  9.9111e+02  1.5549e+02 < 2.2e-16 ***
building_class_at_time_of_saleO8  1.8789e+05  2.4518e+02  7.6634e+02 < 2.2e-16 ***
building_class_at_time_of_saleO9  1.2536e+05  9.2309e+03  1.3580e+01 < 2.2e-16 ***
building_class_at_time_of_saleP1 -5.0139e+05  4.3587e+00 -1.1503e+05 < 2.2e-16 ***
building_class_at_time_of_saleP2  1.6676e+05  4.9800e+01  3.3487e+03 < 2.2e-16 ***
building_class_at_time_of_saleP3  4.0284e+05  9.5488e+00  4.2188e+04 < 2.2e-16 ***
building_class_at_time_of_saleP5  1.1493e+05  3.6320e+01  3.1643e+03 < 2.2e-16 ***
building_class_at_time_of_saleP6  4.5337e+05  1.9562e+01  2.3177e+04 < 2.2e-16 ***
building_class_at_time_of_saleP7  2.5193e+05  3.6547e+00  6.8933e+04 < 2.2e-16 ***
building_class_at_time_of_saleP8 -2.4175e+04  1.0560e+01 -2.2893e+03 < 2.2e-16 ***
building_class_at_time_of_saleP9  9.8652e+04  4.2028e+01  2.3473e+03 < 2.2e-16 ***
building_class_at_time_of_saleQ1 -6.3134e+05  2.0798e+01 -3.0355e+04 < 2.2e-16 ***
building_class_at_time_of_saleQ2 -1.9214e+04  7.9179e+00 -2.4267e+03 < 2.2e-16 ***
building_class_at_time_of_saleQ3 -2.1168e+05  3.9467e+00 -5.3634e+04 < 2.2e-16 ***
building_class_at_time_of_saleQ8  3.3311e+05  5.2588e+00  6.3344e+04 < 2.2e-16 ***
building_class_at_time_of_saleQ9  5.6611e+04  2.5971e+01  2.1798e+03 < 2.2e-16 ***
building_class_at_time_of_saleR0 -6.2189e+04  4.0096e+01 -1.5510e+03 < 2.2e-16 ***
building_class_at_time_of_saleR1 -2.1086e+05  8.5324e+00 -2.4713e+04 < 2.2e-16 ***
building_class_at_time_of_saleR4  1.0408e+05  2.3851e+00  4.3635e+04 < 2.2e-16 ***
building_class_at_time_of_saleR9 -1.3032e+05  7.4740e+01 -1.7436e+03 < 2.2e-16 ***
building_class_at_time_of_saleRR -2.9996e+05  4.0817e+01 -7.3490e+03 < 2.2e-16 ***
building_class_at_time_of_saleS0  1.3380e+05  1.0122e+02  1.3220e+03 < 2.2e-16 ***
building_class_at_time_of_saleS1  1.2850e+05  3.9465e+03  3.2560e+01 < 2.2e-16 ***
building_class_at_time_of_saleS2  1.7599e+05  2.8663e+03  6.1399e+01 < 2.2e-16 ***
building_class_at_time_of_saleS3  1.9130e+05  6.4023e+03  2.9880e+01 < 2.2e-16 ***
building_class_at_time_of_saleS4  2.2695e+05  6.7895e+03  3.3426e+01 < 2.2e-16 ***
building_class_at_time_of_saleS5  2.2889e+05  7.6276e+03  3.0009e+01 < 2.2e-16 ***
building_class_at_time_of_saleS9  2.2762e+05  6.1226e+03  3.7177e+01 < 2.2e-16 ***
building_class_at_time_of_saleT2  2.3426e+04  2.9571e+00  7.9217e+03 < 2.2e-16 ***
building_class_at_time_of_saleT9 -6.9349e+04  8.5254e+00 -8.1344e+03 < 2.2e-16 ***
building_class_at_time_of_saleU7 -1.1867e+05  3.5107e+00 -3.3803e+04 < 2.2e-16 ***
building_class_at_time_of_saleV0 -2.2715e+04  3.7162e+03 -6.1123e+00 9.828e-10 ***
building_class_at_time_of_saleV1 -6.4640e+04  7.6833e+03 -8.4130e+00 < 2.2e-16 ***
building_class_at_time_of_saleV2 -5.6667e+04  7.3045e+01 -7.7578e+02 < 2.2e-16 ***
building_class_at_time_of_saleV3 -6.8084e+04  9.0265e+01 -7.5427e+02 < 2.2e-16 ***
building_class_at_time_of_saleV5  3.5594e+05  4.0220e+00  8.8499e+04 < 2.2e-16 ***
building_class_at_time_of_saleV9 -1.1566e+05  6.3938e+01 -1.8090e+03 < 2.2e-16 ***
building_class_at_time_of_saleW1 -1.3223e+05  2.2746e+01 -5.8133e+03 < 2.2e-16 ***
building_class_at_time_of_saleW2  6.7435e+04  2.2155e+01  3.0438e+03 < 2.2e-16 ***
building_class_at_time_of_saleW3 -4.6763e+04  4.0345e+01 -1.1591e+03 < 2.2e-16 ***
building_class_at_time_of_saleW4 -7.7198e+04  2.8433e+00 -2.7151e+04 < 2.2e-16 ***
building_class_at_time_of_saleW6 -5.9114e+05  5.1179e+00 -1.1550e+05 < 2.2e-16 ***
building_class_at_time_of_saleW7 -3.1586e+05  8.8954e+00 -3.5508e+04 < 2.2e-16 ***
building_class_at_time_of_saleW8  2.2198e+05  2.7235e+01  8.1506e+03 < 2.2e-16 ***
building_class_at_time_of_saleW9  1.4626e+05  4.4200e+01  3.3089e+03 < 2.2e-16 ***
building_class_at_time_of_saleY1 -2.5078e+05  1.5060e+00 -1.6652e+05 < 2.2e-16 ***
building_class_at_time_of_saleY2 -3.4598e+05  7.8875e+00 -4.3864e+04 < 2.2e-16 ***
building_class_at_time_of_saleY4 -3.1975e+05  1.7551e+00 -1.8218e+05 < 2.2e-16 ***
building_class_at_time_of_saleY5 -4.7741e+05  3.0906e+00 -1.5447e+05 < 2.2e-16 ***
building_class_at_time_of_saleY6 -1.1746e+05  1.9593e+01 -5.9949e+03 < 2.2e-16 ***
building_class_at_time_of_saleZ0 -1.3031e+05  4.8316e+01 -2.6969e+03 < 2.2e-16 ***
building_class_at_time_of_saleZ1 -3.3428e+05  3.9923e+00 -8.3731e+04 < 2.2e-16 ***
building_class_at_time_of_saleZ2 -2.1807e+05  2.8958e+01 -7.5306e+03 < 2.2e-16 ***
building_class_at_time_of_saleZ3 -1.2179e+05  3.0810e+00 -3.9531e+04 < 2.2e-16 ***
building_class_at_time_of_saleZ8 -2.0401e+05  1.4632e+00 -1.3942e+05 < 2.2e-16 ***
building_class_at_time_of_saleZ9  4.2683e+04  7.9866e+03  5.3443e+00 9.081e-08 ***
sale_year2003                    -1.0892e+05  1.1024e+03 -9.8802e+01 < 2.2e-16 ***
sale_year2004                    -5.1748e+04  1.0476e+03 -4.9395e+01 < 2.2e-16 ***
sale_year2005                     1.2800e+04  1.0659e+03  1.2008e+01 < 2.2e-16 ***
sale_year2006                     5.6075e+04  1.1214e+03  5.0002e+01 < 2.2e-16 ***
sale_year2007                     5.4919e+04  1.2462e+03  4.4070e+01 < 2.2e-16 ***
sale_year2008                     8.8428e+03  1.3830e+03  6.3940e+00 1.618e-10 ***
sale_year2009                    -3.6373e+04  1.4479e+03 -2.5120e+01 < 2.2e-16 ***
sale_year2010                    -3.7572e+04  1.4401e+03 -2.6089e+01 < 2.2e-16 ***
sale_year2011                    -3.6054e+04  1.5219e+03 -2.3690e+01 < 2.2e-16 ***
sale_year2012                    -2.8094e+04  1.4739e+03 -1.9061e+01 < 2.2e-16 ***
sale_year2013                    -8.9054e+03  1.3906e+03 -6.4040e+00 1.515e-10 ***
sale_year2014                     1.5791e+04  1.3950e+03  1.1320e+01 < 2.2e-16 ***
sale_year2015                     3.2799e+04  1.3994e+03  2.3437e+01 < 2.2e-16 ***
sale_year2016                     6.7075e+04  2.3071e+03  2.9073e+01 < 2.2e-16 ***
sale_year2017                     7.8186e+04  1.7508e+03  4.4658e+01 < 2.2e-16 ***
sale_year2019                     1.3103e+05  1.6770e+03  7.8134e+01 < 2.2e-16 ***
sale_year2020                     1.5247e+05  3.2118e+03  4.7472e+01 < 2.2e-16 ***
year_built0                      -1.3214e+04  4.1824e+03 -3.1593e+00 0.0015815 ** 
year_built1380                   -5.1417e+05  1.2867e+00 -3.9962e+05 < 2.2e-16 ***
year_built1800                   -2.3758e+04  7.1415e+01 -3.3268e+02 < 2.2e-16 ***
year_built1814                   -1.2841e+05  2.4966e+00 -5.1435e+04 < 2.2e-16 ***
year_built1825                   -5.7477e+05  2.6333e+00 -2.1828e+05 < 2.2e-16 ***
year_built1826                    9.3972e+04  4.4656e+00  2.1044e+04 < 2.2e-16 ***
year_built1827                    7.1977e+05  4.6640e+00  1.5432e+05 < 2.2e-16 ***
year_built1838                   -4.0203e+05  3.1975e+00 -1.2573e+05 < 2.2e-16 ***
year_built1839                   -5.6975e+05  2.6778e+00 -2.1277e+05 < 2.2e-16 ***
year_built1840                    2.8993e+04  1.3322e+00  2.1764e+04 < 2.2e-16 ***
 [ reached getOption("max.print") -- omitted 155 rows ]
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.963e+05 on 349122 degrees of freedom
Multiple R-squared: 0.2733,	Adjusted R-squared: 0.2726
> #R-squared value is 25.63%
> 
> # Perform predictions on test set
> reg_pred1 <- predict(fit1, as.matrix(x_test))
> 
> # See the comparison
> results <- data.frame(cbind(y_test, reg_pred1))
> results$diff <- results$reg_pred1 - results$y_test
> results$diff_squared <- '^' (results$diff, 2)
> # RMS = 
> rms = sqrt(mean(results$diff_squared))
> rms
[1] 198239.1
> 
> # RMS Error = 
> rms_error = rms/mean(results$y_test)
> rms_error
[1] 0.4358773
```
</details>

# Machine Learning Modeling

# Shiny App Visualization

This section describes the implmentation of our shiny app visalization, which can be viewed [live on shinyapps.io](https://jcnoordsij.shinyapps.io/nyc_vis/).

## Data Preparation

Before booting up the shiny app, we'll perform a few more processing steps on the data and set up our polygon shapefile for drawing the zip code regions on a leaflet map.

We start by loading in the semi-processed crime data and then filtering out data before 2007, as upon closer inspection, the complaints from before that point seems to have been reported differently and would skew graphs we plan to show in the visualization.

```{r}
load("Data_Score_by_Time_and_Rating.rda")

# Remove NAs and outliers
score_by_time_and_rating = score_by_time_and_rating %>%
    filter(!is.na(weight)) %>%
    filter(month > "2006-12-31")
```

Next, we normalize the crime score by mulitplying by 10000 (to make the minimum score greater than 1), taking it's log, and then dividing by the maximum value to get an approximately normal distribution from 0 to 1.

```{r}
hist(score_by_time_and_rating$weight)

score_by_time_and_rating$weight_transform = log((score_by_time_and_rating$weight)*100000)

score_by_time_and_rating$weight_normalized = score_by_time_and_rating$weight_transform/max(score_by_time_and_rating$weight_transform)

hist(score_by_time_and_rating$weight_normalized)
```

To display the 2015 census data for each zip code, we grab all the unique zip codes from the `score_by_time_and_rating` data frame, and their assciated demographic and economic characteristics.
```{r}
unique_census = score_by_time_and_rating %>%
    group_by(zip_code) %>%
    summarize(
        PerCapitaIncome = PerCapitaIncome[1],
        Unemployed = Unemployed[1],
        TotalPop = TotalPop[1],
        Hispanic =  Hispanic[1],
        White = White[1],
        Black = Black[1], 
        Native = Native[1], 
        Asian = Asian[1],
        weight = weight[1]
    )
```

```{r echo = FALSE}
library(knitr)
library(kableExtra)
kable(head(unique_census, 50), caption = "<p style=\"padding-left:10px\"> **unique_census**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "300px")
```
<br/>
Now that we've extracted the census data, we can remove all those unecessary columns from the `score_by_time_and_rating` and reformat month column as a character to match the style of the `grouped_housing` data frame. We save this as a new data frame that we'll load into our Shiny app.
```{r}
crime_scores = score_by_time_and_rating %>%
    mutate(month_char = format(as.Date(month), "%Y-%m"))%>%
    data.frame() %>%
    select(-c("weight","weight_transform", "sum_weight","Men", "Women", "Hispanic", "White", "Black", "Native", "Asian", "TotalPop", "PerCapitaIncome", "Unemployed"))

save(crime_scores, file = "crime_scores.rda")
```

```{r echo = FALSE}
library(knitr)
library(kableExtra)
kable(head(crime_scores, 50), caption = "<p style=\"padding-left:10px\"> **crime_scores**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "100%", height = "300px")
```
<br/>

    
To display the neighborhood names for each zip code (if they have one), we'll pull them in from a csv file created using [information from the New York State's Department of Health](https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm).

```{r}
neighborhoods = read_csv("neighborhoods.csv") %>%
    mutate(zips = sapply(zips, function(x) as.list(strsplit(x," ")))) %>%
    unnest(zips) %>%
    add_row(zips = "00083", neighborhood = "Central Park")
```

```{r echo = FALSE}
library(knitr)
library(kableExtra)
kable(head(neighborhoods, 50), caption = "<p style=\"padding-left:10px\"> **neighborhoods**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "50%", height = "300px")
```
<br/>

Finally, we'll load in the shapefile that contains all of the zip code polygins (with `st_read`) and merge it with the `neighborhoods` key and the 2015 census data. Before we saving it as an RDA file also we reduce the number of polygon vertices using `ms_simplify()'. This will allow for better performance of our Shiny app.
```{r}
library(jsonlite)
library(sf)

zip_sf = st_read("https://raw.githubusercontent.com/fedhere/PUI2015_EC/master/mam1612_EC/nyc-zip-code-tabulation-areas-polygons.geojson", stringsAsFactors = FALSE) %>%
    select(OBJECTID, postalCode, geometry) %>%
    rename(postalcode = postalCode, shape_id = OBJECTID)
    
zip_sf = merge(zip_sf, unique_census, by.x="postalcode", by.y="zip_code", all.x=TRUE)
zip_sf = merge(zip_sf, neighborhoods, by.x="postalcode", by.y="zips", all.x=TRUE)

zip_sf = rmapshaper::ms_simplify(zip_sf, keep = 0.05, keep_shapes=TRUE)

save(zip_sf, file="zip_polygons.rda")
```

## Running the App
In this sub-section, we review the specific files that are run each time we start up an instance of out Shiny app visualization.

### global.R

This file is always executed before server.R and ui.R and allows us run preliminary code and create global variables used by both the server and UI. We begin the file by loading in the `tidyverse` package and creating a temporary directory where we can store the plots we'll be generating when a user clicks a particular polygon.

```{r}
library(tidyverse)

# Create temp folder for popup plots and rdas
folder <- tempfile()
dir.create(folder)
```

Next, we set the location from where we'll be loading our rda files, as this is different depending on whether we're running it locally or on shinyapps.io (in which case the RDA files are stored on dropbox). Before pushing the app to shinyapps.io, one simply has to uncomment the commented out lines so that the app can pull from dropbox rather than just outside its local directory.
```{r}
rda_loc = "../"

# These lines are for hosting the Shiny app on shinyapps.io
# Leave them commented out when you're running on your local machine
# library(rdrop2)
# drop_auth(rdstoken = "token.rds")
# dir_info = drop_dir("Rstudio_cloud/engm_182_project_data")
# mapply(drop_get,
#     path = dir_info$path_display,
#     local_file = paste(folder,dir_info$name,sep="/"),
#     overwrite = rep(TRUE, nrow(dir_info))
# )
# rda_loc = paste0(folder, "/")
```

Now that we've set their location, we can load in the RDA files. We'll also create a `unique_zips` data frame to hold popup titles for each zipcode polygon in the shapefile.

```{r echo = FALSE}
rda_loc = ""
```

```{r warning = FALSE, message=FALSE}
# Load data gropued by month and zip code
load(paste0(rda_loc, "grouped_housing.rda"))
load(paste0(rda_loc, "crime_scores.rda"))
load(paste0(rda_loc, "zip_polygons.rda"))

unique_zips = zip_sf %>% data.frame() %>%
    select(postalcode, neighborhood) %>%
    group_by(postalcode) %>%
    summarize(neighborhood = neighborhood[ifelse(min(which(!is.na(neighborhood)))==Inf,1,min(which(!is.na(neighborhood))))]) %>%
    mutate(select_names = paste0(ifelse(is.na(.data$neighborhood), "", paste0(.data$neighborhood, " - ")), .data$postalcode)) %>%
    data.frame() %>%
    select(-neighborhood)
```

```{r echo = FALSE}
library(knitr)
library(kableExtra)
kable(head(unique_zips, 50), caption = "<p style=\"padding-left:10px\"> **unique_zips**</p>") %>%
    kable_styling() %>%
    scroll_box(width = "60%", height = "300px")
```
<br/>

Next we build a list to hold the radio button choices to select which data the user wants to look at. 
```{r}
# Build list of radio button choice names and values
radioButtonOptions = setNames(
    as.list(c(colnames(grouped_housing)[-c(1:3)],colnames(crime_scores)[3])), # values
    c( # choice names
        "Average price per sq. ft.",
        "Number of sales",
        "Total proceeds from sales",
        "Crime score"
    )
)
```

Finally, we create some custom CSS and HTML that will be used to fix a bug in the display positioning of the `NA` color within the leaflet legend. We found this solution in a [Github issue thread](https://github.com/rstudio/leaflet/issues/615).
```{r warning=FALSE, message=FALSE}
# Legend NA position fix
library(magrittr)
library(htmlwidgets)
css_fix <- "div.info.legend.leaflet-control br {clear: both;}"
html_fix <- as.character(htmltools::tags$style(type = "text/css", css_fix)) 
```


### server.R
Server is the part of shiny application where processing and updating of data, map, predicted values happen. We'll gradually build the code for Sever in steps. The inputIds in ui.R script will be linked in Sever script.


#### Importing necessary libraries

```{r eval = FALSE}

library(shiny)
library(leaflet)
library(sf)
library(RColorBrewer)
library(zoo)
library(scales)
library(ggplot2)
library(caret)
library(randomForest)

```
#### Loading the prediction model files

We'll be using Neural net and Random forest models to compute prices of real estate based on 4 inputs from user.

```{r eval = FALSE}

load("neural_net.rda")
load("random_forest.rda")

```
#### Creating reactive merged dataframe and mapping
We start coding inside shinyServer function. In next steps, we build chunks of code that will go inside shinyServer function. They execute different processes in the app. Final code of server script is given in the end.

In this first step, we create a reactive obeject that changes the merged dateframe as per the user inputs: time selected and property type selected. The merged dataframe has spatial data. renderLeaflet function creates the map and shows NYC as we have put the coordinates.

```{r eval = FALSE}

shinyServer(function(input, output) {

    df <- reactive({
        if (housing_data_select()) {
            data = grouped_housing %>% 
                filter(category == input$prop_category, month_char == input$date_select) %>%
                select(zip_code, disp_data = !!input$data_select)
        } else {
            data = crime_scores %>%
                filter(month_char == input$date_select) %>%
                select(zip_code, disp_data = !!input$data_select)
        }
        tmp = merge(zip_sf, data, by.x="postalcode", by.y="zip_code", all.x=TRUE, no.dups=FALSE)
        return(tmp)
    })
    
    output$map <- renderLeaflet({
        leaflet() %>%
            addTiles() %>%
            setView(-74.0060,40.7128, zoom=11)
    })
    
})

```

#### Mapping the reactive data and adding legend

Now, we add a color palette. Using reverse geocoding, we map the reactive data based on the ZIP code as per the attribute chosen by the user with radioButton and time chosen on sliderInput. We also highlight the ZIP code areas for mouse hover.

```{r eval = FALSE}
    
    observe({
        
        
        tmp = df()
        legend_labels = NULL
        
        pal = colorNumeric(
            palette = rev(brewer.pal(n=9, name = "RdYlGn")),
            domain = NULL
        )
        
        if (all(is.na(tmp$disp_data))) {
            func = labelFormat()
            tmp$disp_data = as.factor(rep("NA", nrow(tmp)))
            pal = colorFactor(palette = "#808080", domain = NULL)
        } else if (input$data_select == "avg_price_per_sqft") {
            func = labelFormat(prefix = " $")
        } else if (input$data_select == "total_proceeds") {
            func = labelFormat(prefix = " $", suffix = "M", transform=function(x) x/1E6)
        } else {
            func = labelFormat(prefix = " ")
        }
        
        leafletProxy("map", data = tmp) %>%
            clearControls() %>%
            clearShapes() %>%
            clearPopups() %>%
            addPolygons(
                fillColor = ~pal(disp_data),
                color = "#b2aeae", # you need to use hex colors
                fillOpacity = 0.7, 
                weight = 1, 
                smoothFactor = 0.2,
                # Highlight neighbourhoods upon mouseover
                highlight = highlightOptions(
                    weight = 3,
                    color = "black",
                    opacity = 1.0
                ),
                layerId = ~shape_id
            ) %>%
            addLegend(
                title=names(which(radioButtonOptions == input$data_select)),
                pal = pal, 
                values = ~disp_data, 
                position = "bottomright", 
                labFormat = func,
                labels = legend_labels
            )
    })
    

```
#### Clicking on ZIP code areas

We create a function observeEvent that will check the click on a ZIP code area. It will show a pop up inside which we can see a graph of the chosen attribute over time. Code for the pop up in shown in the next step.

```{r eval = FALSE}
    # Observe shape clicks to display popup
    observeEvent(input$map_shape_click,{
        event = input$map_shape_click
        if (is.null(event))
            return()
        else {
            zip_clicked = zip_sf$postalcode[zip_sf$shape_id == event$id]
            curr = FALSE
            if (housing_data_select()) {
                data = grouped_housing %>%
                    filter(category == input$prop_category, zip_code == zip_clicked) %>%
                    select(month_char, !!input$data_select)
                title_suffix_str = paste("-", input$prop_category)
                if (input$prop_category == "All") {
                    title_suffix_str = paste(title_suffix_str, "Categories")
                }
                if (input$data_select %in% c("avg_price_per_sqft", "total_proceeds")) {
                    curr = TRUE
                }
            } else {
                data = crime_scores %>%
                    filter(zip_code == zip_clicked) %>%
                    select(month_char, weight_normalized)
                title_suffix_str = "over Time"
            }
            showPopup(data, zip_clicked, event$id, event$lat, event$lng, title_suffix_str, curr)
        }
    })
```

#### Adding pop up

In this step, we add pop up on each ZIP code area showing the graph of the attribute chosen over time. Pop us also shows demographic information from the 2015 census.

```{r eval = FALSE}
showPopup <- function(df, zip, id, lat, lng, suff_str, curr = FALSE) {
        
        # Get plot object and other popup data for the chosen zip code
        zip_data = zip_sf[which(zip_sf$postalcode == zip),]
        if (nrow(df) > 0) {
            y = pull(df, 2)
            x = as.yearmon(pull(df, 1))
            title = paste(names(which(radioButtonOptions == colnames(df)[2])), suff_str)
            ylabel = names(which(radioButtonOptions == colnames(df)[2]))
            
            if (curr == TRUE) {
                if (max(y, na.rm=TRUE) > 1e6) {
                    label = label_number(prefix = "$", suffix = "M", scale = 1e-6)
                } else if (max(y, na.rm = TRUE) > 1e4) {
                    label =  label_number(prefix = "$", suffix = "K", scale = 1e-3)
                } else {
                    label = label_number(prefix = "$")
                }
            } else {
                label = label_number()
            }
            plot = ggplot(data=NULL, aes(x, y)) +
                geom_line() +
                geom_smooth() +
                labs(title = title, y = ylabel, x = "Month")  +
                scale_y_continuous(label = label, expand = c(0,0), limits = c(0, ifelse(colnames(df)[2] == "weight_normalized", 1, max(y, na.rm=TRUE)))) +
                theme_classic() +
                theme(plot.title = element_text(size = 11, hjust = 0.5), axis.title = element_text(size = 10))
    
            # Write svg file to temporary folder
            svg(filename= paste(folder,"plot.svg", sep = "/"),
                width = 500 * 0.01, height = 300 * 0.01)
            print(plot)
            dev.off()
    
            content <- paste(readLines(paste(folder,"plot.svg",sep="/")), collapse = "")
        } else {
            content = ""
        }
        
        if (is.na(zip_data$neighborhood)) {
            neighborhood_str = ""
        } else {
            neighborhood_str = paste(zip_data$neighborhood, " - ")
        }
        # Create popup
        leafletProxy("map") %>%
            addPopups(
                lng,
                lat,
                popup = paste0(
                    "<h4>", neighborhood_str, zip, "</h4><br/>",
                    content, "<br/>",
                    "<b>2015 Census Data</b><br/>",
                    "Per capita income:    $", round(zip_data$PerCapitaIncome),"</b><br/>",
                    "Total Population: ", zip_data$TotalPop, "</b><br/>",
                    "Unemployment rate: ", round(zip_data$Unemployed), "%"
                ),
                layerId = id,
                options = popupOptions(maxWidth = 500)
            )
    }
```
#### Price predictor

In this step, we integrate the predictive models in the shiny app. It takes 4 inputs from the user: Age of the builing, area, sale year and ZIP code. ZIP code pulls demographic data from the 2015 cenus that's in the dataframe. It feeds these inputs in the model chosen by the user and gives a predicted price.

```{r eval = FALSE}
    
    price_text = eventReactive(input$predict_price, {
        if (input$zip != "")
            zip = unique_zips[which(unique_zips$select_names==input$zip), 1]
        else {
            zip = NA
        }
        input_data = data.frame(gross_square_feet = input$sqft, age = input$age, sale_year = input$sale_year, zip_code = zip, stringsAsFactors = FALSE)
        bad_cols = colnames(input_data)[colSums(is.na(input_data)) > 0]
        if (length(bad_cols) == 0){
            zip_data = zip_sf %>%
                filter(postalcode == zip) %>%
                slice(1) %>%
                data.frame() %>%
                select(postalcode, PerCapitaIncome, White, Black, Asian, Hispanic, Native, TotalPop.x = TotalPop, Unemployed, weight) %>%
                inner_join(input_data, by = c("postalcode" = "zip_code")) %>%
                select(-postalcode)
            if (is.na(zip_data)) {return(paste0("<font color=\"#eb4034\">Error: No valid census data for this zip code!</font>"))}
            if (input$model_select == "fit.rf") {
                predicted_val = predict(fit.rf, newdata=zip_data)
            } else if (input$model_select == "fit.nnet") {
                predicted_val = predict(fit.nnet, newdata=zip_data)
            } else if (input$model_select == "fit.bgcrt") {
                predicted_val = predict(fit.bgcrt, newdata=zip_data)
            }
            return(paste0("<br/>", dollar(predicted_val)))
        } else {
            msg = ""
            if ("gross_square_feet" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a square footage!")
            }
            if ("age" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter building age!")
            }
            if ("sale_year" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a sale year!")
            }
            if ("zip_code" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a location!")
            }
            return(paste0("<font color=\"#eb4034\">", msg, "</font>"))
        }
    })

output$price = renderText({
        price_text()
    })

```
#### Choice of attribute

housing_data_select takes a boolean value of either TRUE or FALSE depending on whether the attribute chosen by the user is within the choices provided.

```{r eval = FALSE}

outputOptions(output, "housing_data_select", suspendWhenHidden = FALSE)
    
    output$housing_data_select = reactive({
        return(housing_data_select())
    })
    
    housing_data_select = reactive({
        if (input$data_select %in% colnames(grouped_housing)[-c(1:3)]) {
            return(TRUE)
        } else {
            return(FALSE)
        }
    })
   
    
```
#### Final code of server.R

Now we combine all chunks of code that we saw in steps so far inside the shinyServer function. This is the final script for server.R. This script along with ui.R and Global.R runs the shiny app.

``` {r eval = FALSE}

library(shiny)
library(leaflet)
library(sf)
library(RColorBrewer)
library(zoo)
library(scales)
library(ggplot2)
library(caret)
library(randomForest)

load("neural_net.rda")
# load(paste0(rda_loc, "bagged_cart.rda"))
load("random_forest.rda")

shinyServer(function(input, output) {
    
    output$housing_data_select = reactive({
        return(housing_data_select())
    })
    
    housing_data_select = reactive({
        if (input$data_select %in% colnames(grouped_housing)[-c(1:3)]) {
            return(TRUE)
        } else {
            return(FALSE)
        }
    })
   
    
    output$price = renderText({
        price_text()
    })
    
    price_text = eventReactive(input$predict_price, {
        if (input$zip != "")
            zip = unique_zips[which(unique_zips$select_names==input$zip), 1]
        else {
            zip = NA
        }
        input_data = data.frame(gross_square_feet = input$sqft, age = input$age, sale_year = input$sale_year, zip_code = zip, stringsAsFactors = FALSE)
        bad_cols = colnames(input_data)[colSums(is.na(input_data)) > 0]
        if (length(bad_cols) == 0){
            zip_data = zip_sf %>%
                filter(postalcode == zip) %>%
                slice(1) %>%
                data.frame() %>%
                select(postalcode, PerCapitaIncome, White, Black, Asian, Hispanic, Native, TotalPop.x = TotalPop, Unemployed, weight) %>%
                inner_join(input_data, by = c("postalcode" = "zip_code")) %>%
                select(-postalcode)
            if (is.na(zip_data)) {return(paste0("<font color=\"#eb4034\">Error: No valid census data for this zip code!</font>"))}
            if (input$model_select == "fit.rf") {
                predicted_val = predict(fit.rf, newdata=zip_data)
            } else if (input$model_select == "fit.nnet") {
                predicted_val = predict(fit.nnet, newdata=zip_data)
            } else if (input$model_select == "fit.bgcrt") {
                predicted_val = predict(fit.bgcrt, newdata=zip_data)
            }
            return(paste0("<br/>", dollar(predicted_val)))
        } else {
            msg = ""
            if ("gross_square_feet" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a square footage!")
            }
            if ("age" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter building age!")
            }
            if ("sale_year" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a sale year!")
            }
            if ("zip_code" %in% bad_cols) {
                msg = paste0(msg, "<br/>Error: Please enter a location!")
            }
            return(paste0("<font color=\"#eb4034\">", msg, "</font>"))
        }
    })

    df <- reactive({
        if (housing_data_select()) {
            data = grouped_housing %>% 
                filter(category == input$prop_category, month_char == input$date_select) %>%
                select(zip_code, disp_data = !!input$data_select)
        } else {
            data = crime_scores %>%
                filter(month_char == input$date_select) %>%
                select(zip_code, disp_data = !!input$data_select)
        }
        tmp = merge(zip_sf, data, by.x="postalcode", by.y="zip_code", all.x=TRUE, no.dups=FALSE)
        return(tmp)
    })
    
    output$map <- renderLeaflet({
        leaflet() %>%
            addTiles() %>%
            setView(-74.0060,40.7128, zoom=11)
    })
    
    observe({
        
        
        tmp = df()
        legend_labels = NULL
        
        pal = colorNumeric(
            palette = rev(brewer.pal(n=9, name = "RdYlGn")),
            domain = NULL
        )
        
        if (all(is.na(tmp$disp_data))) {
            func = labelFormat()
            tmp$disp_data = as.factor(rep("NA", nrow(tmp)))
            pal = colorFactor(palette = "#808080", domain = NULL)
        } else if (input$data_select == "avg_price_per_sqft") {
            func = labelFormat(prefix = " $")
        } else if (input$data_select == "total_proceeds") {
            func = labelFormat(prefix = " $", suffix = "M", transform=function(x) x/1E6)
        } else {
            func = labelFormat(prefix = " ")
        }
        
        leafletProxy("map", data = tmp) %>%
            clearControls() %>%
            clearShapes() %>%
            clearPopups() %>%
            addPolygons(
                fillColor = ~pal(disp_data),
                color = "#b2aeae", # you need to use hex colors
                fillOpacity = 0.7, 
                weight = 1, 
                smoothFactor = 0.2,
                # Highlight neighbourhoods upon mouseover
                highlight = highlightOptions(
                    weight = 3,
                    color = "black",
                    opacity = 1.0
                ),
                layerId = ~shape_id
            ) %>%
            addLegend(
                title=names(which(radioButtonOptions == input$data_select)),
                pal = pal, 
                values = ~disp_data, 
                position = "bottomright", 
                labFormat = func,
                labels = legend_labels
            )
    })
    
    
    # Observe shape clicks to display popup
    observeEvent(input$map_shape_click,{
        event = input$map_shape_click
        if (is.null(event))
            return()
        else {
            zip_clicked = zip_sf$postalcode[zip_sf$shape_id == event$id]
            curr = FALSE
            if (housing_data_select()) {
                data = grouped_housing %>%
                    filter(category == input$prop_category, zip_code == zip_clicked) %>%
                    select(month_char, !!input$data_select)
                title_suffix_str = paste("-", input$prop_category)
                if (input$prop_category == "All") {
                    title_suffix_str = paste(title_suffix_str, "Categories")
                }
                if (input$data_select %in% c("avg_price_per_sqft", "total_proceeds")) {
                    curr = TRUE
                }
            } else {
                data = crime_scores %>%
                    filter(zip_code == zip_clicked) %>%
                    select(month_char, weight_normalized)
                title_suffix_str = "over Time"
            }
            showPopup(data, zip_clicked, event$id, event$lat, event$lng, title_suffix_str, curr)
        }
    })
    
    showPopup <- function(df, zip, id, lat, lng, suff_str, curr = FALSE) {
        
        # Get plot object and other popup data for the chosen zip code
        zip_data = zip_sf[which(zip_sf$postalcode == zip),]
        if (nrow(df) > 0) {
            y = pull(df, 2)
            x = as.yearmon(pull(df, 1))
            title = paste(names(which(radioButtonOptions == colnames(df)[2])), suff_str)
            ylabel = names(which(radioButtonOptions == colnames(df)[2]))
            
            if (curr == TRUE) {
                if (max(y, na.rm=TRUE) > 1e6) {
                    label = label_number(prefix = "$", suffix = "M", scale = 1e-6)
                } else if (max(y, na.rm = TRUE) > 1e4) {
                    label =  label_number(prefix = "$", suffix = "K", scale = 1e-3)
                } else {
                    label = label_number(prefix = "$")
                }
            } else {
                label = label_number()
            }
            plot = ggplot(data=NULL, aes(x, y)) +
                geom_line() +
                geom_smooth() +
                labs(title = title, y = ylabel, x = "Month")  +
                scale_y_continuous(label = label, expand = c(0,0), limits = c(0, ifelse(colnames(df)[2] == "weight_normalized", 1, max(y, na.rm=TRUE)))) +
                theme_classic() +
                theme(plot.title = element_text(size = 11, hjust = 0.5), axis.title = element_text(size = 10))
    
            # Write svg file to temporary folder
            svg(filename= paste(folder,"plot.svg", sep = "/"),
                width = 500 * 0.01, height = 300 * 0.01)
            print(plot)
            dev.off()
    
            content <- paste(readLines(paste(folder,"plot.svg",sep="/")), collapse = "")
        } else {
            content = ""
        }
        
        if (is.na(zip_data$neighborhood)) {
            neighborhood_str = ""
        } else {
            neighborhood_str = paste(zip_data$neighborhood, " - ")
        }
        # Create popup
        leafletProxy("map") %>%
            addPopups(
                lng,
                lat,
                popup = paste0(
                    "<h4>", neighborhood_str, zip, "</h4><br/>",
                    content, "<br/>",
                    "<b>2015 Census Data</b><br/>",
                    "Per capita income:    $", round(zip_data$PerCapitaIncome),"</b><br/>",
                    "Total Population: ", zip_data$TotalPop, "</b><br/>",
                    "Unemployment rate: ", round(zip_data$Unemployed), "%"
                ),
                layerId = id,
                options = popupOptions(maxWidth = 500)
            )
    }
    
    outputOptions(output, "housing_data_select", suspendWhenHidden = FALSE)
})


```

### ui.R

User interface is the user facing side of the Shiny application.It contains various display elements such as titlePanel, sidebarLayout, radioButtons, tabPanel etc. Following steps show how we step-wise buit up code for UI.

#### Importing necessary libraries

``` {r eval = FALSE}

library(shiny)
library(leaflet)
library(shinyWidgets)
```
 
#### Creating basic layout of UI and display elements

Now that we have imported libraries, we start adding code inside fluidPage to create display elements.  We'll have user inputs in the sidebarPanel. We'll add those in later steps. Final code of ui script is given in the end.

``` {r eval = FALSE}

shinyUI(fluidPage(
    HTML(html_fix),
    titlePanel("NYC Real Estate and Crime"),
    sidebarLayout(
        sidebarPanel(),
        mainPanel(
            leafletOutput(
                "map",
                width="100%",
                height="90vh"
            )
        ),
        position="left"
    )
))
```   

#### Creating display elements in sidebarLayout

Now we'll start coding inside sidebarLayout to create display elements. We created 2 tabs, first tab for the attribute we want to see at a chosen time. sliderTextInput allows us to choose a specific time. radioButtons links to radioButtonOptions in Global.R file. It allows us to choose the attribute of the data we want to see. conditionalPanel shows real esate based on the "property type" we have chosen from the drop down list. 

``` {r eval = FALSE}

    titlePanel("NYC Real Estate and Crime"),
    sidebarLayout(
        sidebarPanel(
            tabsetPanel(
                tabPanel(
                    "Map Controls",
                    HTML("<br/>"),
                    sliderTextInput(
                        inputId = "date_select",
                        label="Month",
                        choices = sort(unique(c(unique(crime_scores$month_char),unique(grouped_housing$month_char)))),
                        animate=TRUE,
                        selected="2020-03"
                    ),
                    radioButtons(
                        inputId = "data_select",
                        label = "Mapped Data",
                        radioButtonOptions # set up in global.R
                    ),
                    conditionalPanel(
                        condition = "output.housing_data_select == true",
                        selectInput(
                            inputId = "prop_category",
                            label = "Property Category",
                            choices = sort(unique(grouped_housing$category))
                        )
                    )
                )
```
#### Predictive model inputs

Now we create second tab for the predictive model integration in the app. We build radioButtons to choose between models. Price predictor takes 4 inputs to compute the price. Clicking on the actionButton shows predicted price.

``` {r eval = FALSE}

          tabPanel(
                    "Price Predictor",
                    HTML("<br/>"),
                    numericInput("age", "Building age (years):", 50, min=1800, max = 2030),
                    numericInput("sqft", "Square footage:", 2000, min=0, max = 1000000),
                    numericInput("sale_year", "Sale Year:", 2010, min=1950, max = 2030),
                    selectizeInput("zip", "Location:",
                        selected = NULL,
                        choices = unique_zips$select_names,
                        options = list(
                            placeholder = "Search by Neighborhood or Zip Code",
                            onInitialize = I('function() { this.setValue(""); }')
                        )
                    ),
                    radioButtons(
                        "model_select",
                        label="Predictive Model:",
                        choiceNames = c(
                            # "Bagged cart",
                            "Neural net",
                            "Random forest"
                        ),
                        choiceValues = c(
                            # "fit.bgcrt", 
                            "fit.nnet",
                            "fit.rf"
                        )
                    ),
                    actionButton("predict_price", label = "Compute Price"),
                    HTML("<hr/><b>Predicted Price: </b>"),
                    htmlOutput("price")
                )
                
```
#### Final code of ui.R

Now we combine all chunks of code that we saw in steps so far inside the shinyUI function. This is the final script for ui.R. This script along with server.R and Global.R runs the shiny app.

``` {r eval = FALSE}

library(shiny)
library(leaflet)
library(shinyWidgets)

# Define UI for application that draws a histogram
shinyUI(fluidPage(
    HTML(html_fix),
    titlePanel("NYC Real Estate and Crime"),
    sidebarLayout(
        sidebarPanel(
            tabsetPanel(
                tabPanel(
                    "Map Controls",
                    HTML("<br/>"),
                    sliderTextInput(
                        inputId = "date_select",
                        label="Month",
                        choices = sort(unique(c(unique(crime_scores$month_char),unique(grouped_housing$month_char)))),
                        animate=TRUE,
                        selected="2020-03"
                    ),
                    radioButtons(
                        inputId = "data_select",
                        label = "Mapped Data",
                        radioButtonOptions # set up in global.R
                    ),
                    conditionalPanel(
                        condition = "output.housing_data_select == true",
                        selectInput(
                            inputId = "prop_category",
                            label = "Property Category",
                            choices = sort(unique(grouped_housing$category))
                        )
                    )
                ),
                tabPanel(
                    "Price Predictor",
                    HTML("<br/>"),
                    numericInput("age", "Building age (years):", 50, min=1800, max = 2030),
                    numericInput("sqft", "Square footage:", 2000, min=0, max = 1000000),
                    numericInput("sale_year", "Sale Year:", 2010, min=1950, max = 2030),
                    selectizeInput("zip", "Location:",
                        selected = NULL,
                        choices = unique_zips$select_names,
                        options = list(
                            placeholder = "Search by Neighborhood or Zip Code",
                            onInitialize = I('function() { this.setValue(""); }')
                        )
                    ),
                    radioButtons(
                        "model_select",
                        label="Predictive Model:",
                        choiceNames = c(
                            # "Bagged cart",
                            "Neural net",
                            "Random forest"
                        ),
                        choiceValues = c(
                            # "fit.bgcrt", 
                            "fit.nnet",
                            "fit.rf"
                        )
                    ),
                    actionButton("predict_price", label = "Compute Price"),
                    HTML("<hr/><b>Predicted Price: </b>"),
                    htmlOutput("price")
                )
            )
        ),
        mainPanel(
            leafletOutput(
                "map",
                width="100%",
                height="90vh"
            )
        ),
        position="left"
    )
))
    


```
### Conclusion and next steps

#### Challenges faced

* Selecting relevant variables

    Many factors go into predicting the housing prices. So it's difficult to select a set of variables from the data available.

*  Cleaning the data

    Cleaning the raw data is an important step. Often, the data collected is not in the required shape so a lot wrangling has to be done before it's ready for further processing.

*  Missing data

    There were many data points that were missing. We couldn't do much about them. There are many ways to deal with missing data. However, they were not within the scope of the project. Grey areas seen on the map are due to these missing data points.

*  Computing power limitations
    We experienced computing power limitations while working with machine learning models. In future, these models can be run with better computing.

#### What we learned

* Data cleaning takes time

    Cleaning and preparing the data into required form is an important step and can be time consuming. Often, it takes more time to clean and prepare the date than processing it.

* Higher computing or APIs needed to run the models

    To run the machine learning models, higher computing power would be useful. Othewise, using some API package like rcpp the code could be executed faster.


* Lost insights due to missing data
    We may have missed out on many insights due to missing data. There was nothing much we could do about this. In future,  missing data could be handled better.









